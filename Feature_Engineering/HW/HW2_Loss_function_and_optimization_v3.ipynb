{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a06f38f-1617-4c21-8b21-939e12bc0629",
   "metadata": {},
   "source": [
    "## Домашнее задание к занятию «Классификация: «Функции потерь и оптимизация»\n",
    "\n",
    "### Задание\n",
    "\n",
    "<b>Цель:</b><br> изучить применение методов оптимизации для решения задачи классификации<p>\n",
    "<b>Описание задания:</b><br>\n",
    "В домашнем задании необходимо применить полученные знания в теории оптимизации и машинном обучении для реализации логистической регрессии.<p>\n",
    "\n",
    "<b>Этапы работы:</b><p>\n",
    "\n",
    "1. Загрузите данные. Используйте <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\" target=\"_blank\">датасет</a> с ирисами. Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только <b>2</b> класса: <u>Iris Versicolor, Iris Virginica.</u>\n",
    "2. Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. Можете использовать библиотеки pandas, numpy, math для реализации. Оформите в виде функции. *Оформите в виде класса с методами.\n",
    "3. Реализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. Выберете и посчитайте метрику качества. Метрика должна быть одинакова для всех пунктов домашнего задания. Для упрощения сравнения выберете только одну метрику.\n",
    "4. Повторите п. 3 для метода скользящего среднего (Root Mean Square Propagation, RMSProp).\n",
    "5. Повторите п. 3 для ускоренного по Нестерову метода адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam).\n",
    "6. Сравните значение метрик для реализованных методов оптимизации. Можно оформить в виде таблицы вида |метод|метрика|время работы| (время работы опционально). Напишите вывод.<p>\n",
    "\n",
    "Для лучшего понимания темы и упрощения реализации можете обратиться к <a href=\"https://habr.com/en/post/318970/\" target=\"_blank\">статье</a>.\n",
    "\n",
    "Для получение зачета по этому домашнему заданию, минимально, должно быть реализовано обучение логистической регрессии и градиентный спуск.\n",
    "\n",
    "<b>Результат:</b> получены навыки реализации методов оптимизации в задаче бинарной классификации. Пройденные методы оптимизации используются и в нейросетях.<p>\n",
    "<b>Форма выполнения:</b> ссылка на Jupyter Notebook, загруженный на GitHub; ссылка на Google Colab; файл с расширением .ipynb.<p>\n",
    "<b>Инструменты:</b> Jupyter Notebook/Google Colab; GitHub. <p>\n",
    "\n",
    "<b><i>Рекомендации к выполнению: </b></i><br>\n",
    "- Текст оформляйте в отдельной ячейке Jupyter Notebook/Google Colab в формате markdown.\n",
    "- У графиков должен быть заголовок, подписи осей, легенда (опционально). Делайте графики бОльшего размера, чем стандартный вывод, чтобы увеличить читаемость.\n",
    "- Убедитесь, что по ссылкам есть доступ на чтение/просмотр.\n",
    "- Убедитесь, что все ячейки в работе выполнены и можно увидеть их вывод без повторного запуска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2a6d0-6cf2-4e5a-94f8-6dab4879487d",
   "metadata": {},
   "source": [
    "### Решение:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13ff85-f428-4217-8654-071638ce4855",
   "metadata": {},
   "source": [
    "> Импортируем библиотеки и датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9d4072-9546-4a9b-9353-900c8cdff52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8610bd3e-fb77-4cbb-8747-39fe47342aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасета iris\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X = iris.data[iris.target != 0] # убираем все данные для класса 0 из X\n",
    "y = iris.target[iris.target != 0] # убираем все данные для класса 0 из Y\n",
    "y[y==2] = 0 # Изменим метки классов на 0 и 1 (заменив 2 на 0)\n",
    "X = np.insert(X, 0, 1, axis=1) # Добавим столбец с единицами для обучения theta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b348563-97c5-4071-9493-b63449f5138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataFrame из датасета iris\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df = df[df['target'] != 0]  # оставляем только Iris Versicolor и Iris Virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fcf62-2827-4939-b905-bf647fa51475",
   "metadata": {},
   "source": [
    "> Разделяем набор данных на наборы для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d91324-195c-4d3c-b2d4-0917ad7c721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ae715-36c2-4ba4-8abc-07e18f6b330d",
   "metadata": {},
   "source": [
    "> Создаем класс LogisticRegression: реализация логистической регрессии методом градиентного спукса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee27f4d-c008-4777-9c49-3984e8406a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, num_iterations=3000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        m = len(y)\n",
    "        for i in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h-y))/m\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.sigmoid(np.dot(X, self.theta))\n",
    "        return [1 if x >= 0.5 else 0 for x in prob]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393aa76-8b6e-40b7-9e4c-0a3ef209b98d",
   "metadata": {},
   "source": [
    "> accuracy_score - метрика качетсва вручную и с помощью библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9462780d-e1c0-4edc-92ac-d8da0d15bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_pred):\n",
    "    return np.mean(y_test == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e97278f-3a09-4035-a0d7-9b611774123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred1 = LR.predict(X_test)\n",
    "acc1 = accuracy(y_test, y_pred1)\n",
    "print('Accuracy:', acc1)\n",
    "end_time = time.time()\n",
    "execution_time1 = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79334fea-b211-4be0-a8e6-5cb700ee3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "print('Accuracy:', accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c3fee-d6d4-4626-bdcd-fada3579e7c0",
   "metadata": {},
   "source": [
    "<b><i>Вывод 1</b> - Дали одинаковый результат, буду использовать библиотеку</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4613bfc-9fbf-4f42-9b7a-721b254f237c",
   "metadata": {},
   "source": [
    "> Метод скользящего среднего (Root Mean Square Propagation, RMSProp) <a href=\"https://puzzlelib.org/ru/documentation/base/optimizers/RMSProp/#:~:text=RMSProp%20\" target=\"_blank\">Описание метода RMSProp </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f38f5f-b265-4ce8-84a8-b188810d0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta=0.9, epsilon=1e-8, num_iterations=3000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        v = np.zeros(X.shape[1])\n",
    "        m = len(y)\n",
    "        for i in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h-y))/m\n",
    "            v = self.beta * v + (1 - self.beta) * np.square(gradient)\n",
    "            self.theta -= self.learning_rate * gradient / (np.sqrt(v) + self.epsilon)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.sigmoid(np.dot(X, self.theta))\n",
    "        return [1 if x >= 0.5 else 0 for x in prob]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d448b0-b8e0-4417-adfd-7901b18d46ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "RMS = LogisticRegression()\n",
    "RMS.fit(X_train, y_train)\n",
    "y_pred2 = RMS.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "print('Accuracy:', accuracy2)\n",
    "end_time = time.time()\n",
    "execution_time2 = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e3cd7-dbaa-4072-92c3-0fca4df736d4",
   "metadata": {},
   "source": [
    "> Метод адаптивной оценки моментов по Нестерову (Nesterov–accelerated Adaptive Moment Estimation, Nadam). <a href=\"https://proproprogs.ru/neural_network/optimizatory-v-keras-formirovanie-vyborki-validacii\" target=\"_blank\">Описание метода </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf03beb-0ef9-4be5-b0be-97d619cf94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nadam:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=3000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        m_hat = np.zeros(X.shape[1]) # Используется для коррекции смещения на 1-ом шаге оптимизации\n",
    "        v_hat = np.zeros(X.shape[1])\n",
    "        m = len(y)\n",
    "        for i in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h-y))/m\n",
    "            m_hat = self.beta1 * m_hat + (1 - self.beta1) * gradient\n",
    "            v_hat = self.beta2 * v_hat + (1 - self.beta2) * np.square(gradient)\n",
    "            m_hat_corr = m_hat / (1 - np.power(self.beta1, i+1))\n",
    "            v_hat_corr = v_hat / (1 - np.power(self.beta2, i+1))\n",
    "            self.theta -= self.learning_rate * m_hat_corr / (np.sqrt(v_hat_corr) + self.epsilon)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.sigmoid(np.dot(X, self.theta))\n",
    "        return [1 if x >= 0.5 else 0 for x in prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862f61c0-e358-4072-846d-83919431f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "Nad = Nadam()\n",
    "Nad.fit(X_train, y_train)\n",
    "y_pred3 = Nad.predict(X_test)\n",
    "accuracy3 = accuracy_score(y_test, y_pred3)\n",
    "print('Accuracy:', accuracy3)\n",
    "end_time = time.time()\n",
    "execution_time3 = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b86dbe-db3d-41f5-9143-0be43bd05e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Метод оптимизации | Точность | время выполнения  |\n",
      "|-------------------|----------|\n",
      "| Градиентный спуск | 0.9333   |  0.0431  |\n",
      "| RMSProp           | 0.9333   |  0.0505  |\n",
      "| Nadam             | 0.9000   |  0.0826  |\n"
     ]
    }
   ],
   "source": [
    "print('| Метод оптимизации | Точность | время выполнения  |')\n",
    "print('|-------------------|----------|')\n",
    "print(f'| Градиентный спуск | {accuracy1:.4f}   |  {execution_time1:.4f}  |')\n",
    "print(f'| RMSProp           | {accuracy2:.4f}   |  {execution_time2:.4f}  |')\n",
    "print(f'| Nadam             | {accuracy3:.4f}   |  {execution_time3:.4f}  |')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2f37f-dba8-4eaf-9bd8-0d2c7be5559f",
   "metadata": {},
   "source": [
    "> <b>Вывод:</b> <br>Из таблицы видно, что Градиентый спуск работает быстрее, RMSProp и Градиентый спуск более точныею       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
